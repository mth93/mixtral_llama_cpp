{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-12T12:49:45.486398Z","iopub.status.busy":"2023-12-12T12:49:45.485708Z","iopub.status.idle":"2023-12-12T12:52:14.923110Z","shell.execute_reply":"2023-12-12T12:52:14.922036Z","shell.execute_reply.started":"2023-12-12T12:49:45.486366Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle\n","Cloning into 'llama.cpp'...\n","remote: Enumerating objects: 13210, done.\u001b[K\n","remote: Counting objects: 100% (3778/3778), done.\u001b[K\n","remote: Compressing objects: 100% (419/419), done.\u001b[K\n","remote: Total 13210 (delta 3540), reused 3488 (delta 3358), pack-reused 9432\u001b[K\n","Receiving objects: 100% (13210/13210), 15.77 MiB | 24.92 MiB/s, done.\n","Resolving deltas: 100% (9182/9182), done.\n","/kaggle/llama.cpp\n","I llama.cpp build info: \n","I UNAME_S:   Linux\n","I UNAME_P:   x86_64\n","I UNAME_M:   x86_64\n","I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native \n","I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native \n","I NVCCFLAGS: --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native \"\n","I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n","I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n","\n","cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml.c -o ggml.o\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c llama.cpp -o llama.o\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/common.cpp -o common.o\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/sampling.cpp -o sampling.o\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/grammar-parser.cpp -o grammar-parser.o\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/build-info.cpp -o build-info.o\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/console.cpp -o console.o\n","nvcc --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native \" -c ggml-cuda.cu -o ggml-cuda.o\n","cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml-alloc.c -o ggml-alloc.o\n","cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native    -c ggml-backend.c -o ggml-backend.o\n","cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native     -c ggml-quants.c -o ggml-quants.o\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","\n","====  Run ./main -h for help.  ====\n","\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  pocs/vdot/vdot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  pocs/vdot/q8dot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -c common/train.cpp -o train.o\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib   -Wno-cast-qual\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/gguf/gguf.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib  -Wno-cast-qual\n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/export-lora/export-lora.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -mtune=native  examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native  -c tests/test-c.c -o tests/test-c.o\n"]}],"source":["%cd /kaggle\n","!git clone -b mixtral https://github.com/ggerganov/llama.cpp.git\n","%cd llama.cpp\n","!make LLAMA_CUBLAS=1\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T12:52:14.925349Z","iopub.status.busy":"2023-12-12T12:52:14.925017Z","iopub.status.idle":"2023-12-12T13:05:47.817794Z","shell.execute_reply":"2023-12-12T13:05:47.816656Z","shell.execute_reply.started":"2023-12-12T12:52:14.925318Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting numpy==1.24.4 (from -r requirements.txt (line 1))\n","  Obtaining dependency information for numpy==1.24.4 from https://files.pythonhosted.org/packages/10/be/ae5bf4737cb79ba437879915791f6f26d92583c738d7d960ad94e5c36adf/numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n","  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Collecting sentencepiece==0.1.98 (from -r requirements.txt (line 2))\n","  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting gguf>=0.1.0 (from -r requirements.txt (line 3))\n","  Obtaining dependency information for gguf>=0.1.0 from https://files.pythonhosted.org/packages/38/c1/d9f7b99aee449a83d1453c37256e78cbbb90eeab0a2a83700082df970fb6/gguf-0.5.1-py3-none-any.whl.metadata\n","  Downloading gguf-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n","Collecting torch==2.1.1 (from -r requirements-hf-to-gguf.txt (line 2))\n","  Obtaining dependency information for torch==2.1.1 from https://files.pythonhosted.org/packages/96/82/0966469ded5946cb4c18dd11b04eac78c943269fc79d290740d6477005e8/torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata\n","  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n","Requirement already satisfied: transformers==4.35.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements-hf-to-gguf.txt (line 3)) (4.35.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2)) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2)) (4.5.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2)) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2)) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2)) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2)) (2023.12.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/nvidia-nccl-cu12/\u001b[0m\u001b[33m\n","\u001b[0mCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting triton==2.1.0 (from torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Obtaining dependency information for triton==2.1.0 from https://files.pythonhosted.org/packages/4d/22/91a8af421c8a8902dde76e6ef3db01b258af16c53d81e8c0d0dc13900a9e/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n","  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (0.19.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (2023.8.8)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (4.66.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2))\n","  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/1e/07/bf730d44c2fe1b676ad9cc2be5f5f861eb5d153fb6951987a2d6a96379a9/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata\n","  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (3.0.9)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2)) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35.2->-r requirements-hf-to-gguf.txt (line 3)) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.1->-r requirements-hf-to-gguf.txt (line 2)) (1.3.0)\n","Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m927.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:15\u001b[0m\n","\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n","\u001b[?25hDownloading gguf-0.5.1-py3-none-any.whl (23 kB)\n","Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hInstalling collected packages: sentencepiece, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, nvidia-cusolver-cu12, torch\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.99\n","    Uninstalling sentencepiece-0.1.99:\n","      Successfully uninstalled sentencepiece-0.1.99\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.24.3\n","    Uninstalling numpy-1.24.3:\n","      Successfully uninstalled numpy-1.24.3\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.0\n","    Uninstalling torch-2.0.0:\n","      Successfully uninstalled torch-2.0.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\n","cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\n","dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\n","dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\n","dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\n","dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n","libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n","pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.4 which is incompatible.\n","pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\n","raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\n","raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\n","tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.24.4 which is incompatible.\n","tensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\n","ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gguf-0.5.1 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 sentencepiece-0.1.98 torch-2.1.1 triton-2.1.0\n"]}],"source":["!pip install -r requirements-hf-to-gguf.txt"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T13:05:47.819921Z","iopub.status.busy":"2023-12-12T13:05:47.819537Z","iopub.status.idle":"2023-12-12T13:08:13.612967Z","shell.execute_reply":"2023-12-12T13:08:13.611477Z","shell.execute_reply.started":"2023-12-12T13:05:47.819882Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-12-12 13:05:48--  https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q5_0.gguf\n","Resolving huggingface.co (huggingface.co)... 65.8.243.46, 65.8.243.92, 65.8.243.16, ...\n","Connecting to huggingface.co (huggingface.co)|65.8.243.46|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs-us-1.huggingface.co/repos/30/b6/30b69c633b55e4c2fc9aea4badf4dbd92c604ded097593b40f9e038cb22808c4/59ac8096b551c7e11e23a1b56ffeaca10b0441070d6edcecfa023c8edd78604c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mixtral-8x7b-v0.1.Q5_0.gguf%3B+filename%3D%22mixtral-8x7b-v0.1.Q5_0.gguf%22%3B&Expires=1702645548&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjY0NTU0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzMwL2I2LzMwYjY5YzYzM2I1NWU0YzJmYzlhZWE0YmFkZjRkYmQ5MmM2MDRkZWQwOTc1OTNiNDBmOWUwMzhjYjIyODA4YzQvNTlhYzgwOTZiNTUxYzdlMTFlMjNhMWI1NmZmZWFjYTEwYjA0NDEwNzBkNmVkY2VjZmEwMjNjOGVkZDc4NjA0Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=CqRrCh6UG-eJG5ncznvqtEFyYrBLH2nTnNaJITU-RfJSpI%7E1UtwBJd46EcqjXUf%7E1oVFM70Wu8lDe2vNYhm0ylvo-Hw7SLlDl69U4Cd8G5oqzOW37wHX0dQK1cKMxCqzDRPCDlnpn-A7ZtAX3OJ27%7ErxcZsRqZiMwewJ4s6bju-cWxhZWd94PSNp7vdExoz3yw8NpyxCtkztE13AS2zHKMaaR%7ENam6fcqjjgGF7Sogsh9WAcP1-u4sarpAGvzSVjDp5MzF1Kpze158wsfN6IJYOhmW7aX5WYHcGRWDzcMNKd3LuN-Q0Yul0BPa13CpC5ZAyltc7xU6v4AFDVKZ7LtQ__&Key-Pair-Id=KCD77M1F0VK2B [following]\n","--2023-12-12 13:05:49--  https://cdn-lfs-us-1.huggingface.co/repos/30/b6/30b69c633b55e4c2fc9aea4badf4dbd92c604ded097593b40f9e038cb22808c4/59ac8096b551c7e11e23a1b56ffeaca10b0441070d6edcecfa023c8edd78604c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mixtral-8x7b-v0.1.Q5_0.gguf%3B+filename%3D%22mixtral-8x7b-v0.1.Q5_0.gguf%22%3B&Expires=1702645548&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjY0NTU0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzMwL2I2LzMwYjY5YzYzM2I1NWU0YzJmYzlhZWE0YmFkZjRkYmQ5MmM2MDRkZWQwOTc1OTNiNDBmOWUwMzhjYjIyODA4YzQvNTlhYzgwOTZiNTUxYzdlMTFlMjNhMWI1NmZmZWFjYTEwYjA0NDEwNzBkNmVkY2VjZmEwMjNjOGVkZDc4NjA0Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=CqRrCh6UG-eJG5ncznvqtEFyYrBLH2nTnNaJITU-RfJSpI%7E1UtwBJd46EcqjXUf%7E1oVFM70Wu8lDe2vNYhm0ylvo-Hw7SLlDl69U4Cd8G5oqzOW37wHX0dQK1cKMxCqzDRPCDlnpn-A7ZtAX3OJ27%7ErxcZsRqZiMwewJ4s6bju-cWxhZWd94PSNp7vdExoz3yw8NpyxCtkztE13AS2zHKMaaR%7ENam6fcqjjgGF7Sogsh9WAcP1-u4sarpAGvzSVjDp5MzF1Kpze158wsfN6IJYOhmW7aX5WYHcGRWDzcMNKd3LuN-Q0Yul0BPa13CpC5ZAyltc7xU6v4AFDVKZ7LtQ__&Key-Pair-Id=KCD77M1F0VK2B\n","Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.156.107.49, 108.156.107.80, 108.156.107.29, ...\n","Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.156.107.49|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 32229279168 (30G) [binary/octet-stream]\n","Saving to: ‘mixtral-8x7b-v0.1.Q5_0.gguf’\n","\n","mixtral-8x7b-v0.1.Q 100%[===================>]  30.02G   232MB/s    in 2m 24s  \n","\n","2023-12-12 13:08:13 (213 MB/s) - ‘mixtral-8x7b-v0.1.Q5_0.gguf’ saved [32229279168/32229279168]\n","\n"]}],"source":["!wget \"https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q5_0.gguf\""]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T13:08:13.616479Z","iopub.status.busy":"2023-12-12T13:08:13.616150Z","iopub.status.idle":"2023-12-12T13:08:22.298761Z","shell.execute_reply":"2023-12-12T13:08:22.297186Z","shell.execute_reply.started":"2023-12-12T13:08:13.616448Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-12-12 13:08:14--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n","Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 52.202.168.65, 18.205.222.128, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8812405 (8.4M) [application/octet-stream]\n","Saving to: ‘ngrok-v3-stable-linux-amd64.tgz’\n","\n","ngrok-v3-stable-lin 100%[===================>]   8.40M  30.0MB/s    in 0.3s    \n","\n","2023-12-12 13:08:15 (30.0 MB/s) - ‘ngrok-v3-stable-linux-amd64.tgz’ saved [8812405/8812405]\n","\n","ngrok\n"]}],"source":["!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n","!tar -xvzf ngrok-v3-stable-linux-amd64.tgz"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T13:08:22.300778Z","iopub.status.busy":"2023-12-12T13:08:22.300456Z","iopub.status.idle":"2023-12-12T13:08:24.257908Z","shell.execute_reply":"2023-12-12T13:08:24.256367Z","shell.execute_reply.started":"2023-12-12T13:08:22.300749Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}],"source":["!./ngrok config add-authtoken <token>"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T13:08:24.261254Z","iopub.status.busy":"2023-12-12T13:08:24.260195Z","iopub.status.idle":"2023-12-12T13:08:25.614415Z","shell.execute_reply":"2023-12-12T13:08:25.613315Z","shell.execute_reply.started":"2023-12-12T13:08:24.261221Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Server started in the background. Log file: server.log\n","Background process ID: 395\n"]}],"source":["\n","\n","import subprocess\n","import platform\n","\n","def is_windows():\n","    return platform.system().lower() == 'windows'\n","\n","# Specify the command with additional parameters\n","command = [\n","    './server.exe' if is_windows() else './server',  # Adjust for Windows executable\n","    '-m', 'mixtral-8x7b-v0.1.Q5_0.gguf',\n","    '-c', '4096',\n","    '--host', '0.0.0.0',  # Listen on all network interfaces\n","    '--port', '8080',      # Specify the port for API access\n","    '--n-gpu-layers', '30',# Utilize GPU for computation (requires appropriate support)\n","]\n","\n","# Add output redirection to a log file\n","command_str = ' '.join(command) + ' > server.log 2>&1 &'\n","\n","# Run the command in the background\n","process = subprocess.Popen(command_str, shell=True)\n","\n","# Print a message indicating the server has started\n","print(f\"Server started in the background. Log file: server.log\")\n","\n","# Optionally, print the process ID (PID)\n","print(\"Background process ID:\", process.pid)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T13:08:25.616646Z","iopub.status.busy":"2023-12-12T13:08:25.615962Z","iopub.status.idle":"2023-12-12T13:08:25.694666Z","shell.execute_reply":"2023-12-12T13:08:25.693371Z","shell.execute_reply.started":"2023-12-12T13:08:25.616609Z"},"trusted":true},"outputs":[],"source":["\n","from IPython import get_ipython\n","\n","get_ipython().system_raw(\"./ngrok http 8080 &\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T13:08:25.697257Z","iopub.status.busy":"2023-12-12T13:08:25.696525Z","iopub.status.idle":"2023-12-12T13:08:27.109871Z","shell.execute_reply":"2023-12-12T13:08:27.108402Z","shell.execute_reply.started":"2023-12-12T13:08:25.697220Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["https://523f-104-197-71-232.ngrok-free.app\n"]}],"source":["!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T13:08:27.112368Z","iopub.status.busy":"2023-12-12T13:08:27.111911Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/llama.cpp\n","/kaggle/llama.cpp\n","ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n","ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n","ggml_init_cublas: found 2 CUDA devices:\n","  Device 0: Tesla T4, compute capability 7.5\n","  Device 1: Tesla T4, compute capability 7.5\n","{\"timestamp\":1702386510,\"level\":\"INFO\",\"function\":\"main\",\"line\":2652,\"message\":\"build info\",\"build\":1660,\"commit\":\"08eb991\"}\n","{\"timestamp\":1702386510,\"level\":\"INFO\",\"function\":\"main\",\"line\":2655,\"message\":\"system info\",\"n_threads\":4,\"n_threads_batch\":-1,\"total_threads\":4,\"system_info\":\"AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \"}\n","llama_model_loader: loaded meta data with 25 key-value pairs and 995 tensors from mixtral-8x7b-v0.1.Q5_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: - tensor    0:                token_embd.weight q5_0     [  4096, 32000,     1,     1 ]\n","llama_model_loader: - tensor    1:          blk.0.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor    2:          blk.0.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor    3:            blk.0.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor    4:          blk.0.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor    5:          blk.0.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor    6:            blk.0.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor    7:          blk.0.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor    8:          blk.0.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor    9:            blk.0.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   10:          blk.0.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   11:          blk.0.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   12:            blk.0.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   13:          blk.0.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   14:          blk.0.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   15:            blk.0.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   16:          blk.0.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   17:          blk.0.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   18:            blk.0.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   19:          blk.0.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   20:          blk.0.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   21:            blk.0.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   22:          blk.0.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   23:          blk.0.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   24:            blk.0.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   25:        blk.0.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor   26:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor   27:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor   28:              blk.0.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor   29:         blk.0.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor   30:              blk.0.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor   31:              blk.0.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor   32:          blk.1.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   33:          blk.1.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   34:            blk.1.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   35:          blk.1.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   36:          blk.1.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   37:            blk.1.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   38:          blk.1.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   39:          blk.1.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   40:            blk.1.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   41:          blk.1.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   42:          blk.1.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   43:            blk.1.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   44:          blk.1.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   45:          blk.1.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   46:        blk.1.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor   47:              blk.1.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor   48:         blk.1.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor   49:              blk.1.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor   50:              blk.1.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor   51:            blk.1.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   52:          blk.1.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   53:          blk.1.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   54:            blk.1.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   55:          blk.1.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   56:          blk.1.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   57:            blk.1.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   58:          blk.1.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   59:          blk.1.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   60:            blk.1.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   61:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor   62:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor   63:          blk.2.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   64:          blk.2.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   65:            blk.2.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   66:          blk.2.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   67:          blk.2.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   68:            blk.2.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   69:          blk.2.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   70:          blk.2.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   71:            blk.2.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   72:          blk.2.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   73:          blk.2.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   74:            blk.2.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   75:          blk.2.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   76:          blk.2.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   77:            blk.2.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   78:          blk.2.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   79:          blk.2.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   80:            blk.2.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   81:          blk.2.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   82:          blk.2.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   83:            blk.2.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   84:          blk.2.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   85:          blk.2.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   86:            blk.2.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   87:        blk.2.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor   88:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor   89:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor   90:              blk.2.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor   91:         blk.2.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor   92:              blk.2.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor   93:              blk.2.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor   94:          blk.3.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   95:          blk.3.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   96:            blk.3.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   97:          blk.3.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor   98:          blk.3.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor   99:            blk.3.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  100:          blk.3.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  101:        blk.3.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  102:              blk.3.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  103:         blk.3.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  104:              blk.3.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  105:              blk.3.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  106:          blk.3.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  107:            blk.3.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  108:          blk.3.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  109:          blk.3.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  110:            blk.3.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  111:          blk.3.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  112:          blk.3.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  113:            blk.3.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  114:          blk.3.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  115:          blk.3.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  116:            blk.3.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  117:          blk.3.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  118:          blk.3.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  119:            blk.3.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  120:          blk.3.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  121:          blk.3.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  122:            blk.3.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  123:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  124:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  125:          blk.4.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  126:          blk.4.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  127:            blk.4.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  128:          blk.4.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  129:          blk.4.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  130:            blk.4.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  131:          blk.4.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  132:          blk.4.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  133:            blk.4.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  134:          blk.4.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  135:          blk.4.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  136:            blk.4.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  137:          blk.4.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  138:          blk.4.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  139:            blk.4.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  140:          blk.4.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  141:          blk.4.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  142:            blk.4.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  143:          blk.4.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  144:          blk.4.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  145:            blk.4.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  146:          blk.4.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  147:          blk.4.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  148:            blk.4.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  149:        blk.4.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  150:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  151:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  152:              blk.4.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  153:         blk.4.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  154:              blk.4.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  155:              blk.4.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  156:        blk.5.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  157:              blk.5.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  158:         blk.5.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  159:              blk.5.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  160:              blk.5.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  161:          blk.5.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  162:          blk.5.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  163:            blk.5.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  164:          blk.5.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  165:          blk.5.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  166:            blk.5.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  167:          blk.5.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  168:          blk.5.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  169:            blk.5.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  170:          blk.5.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  171:          blk.5.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  172:            blk.5.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  173:          blk.5.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  174:          blk.5.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  175:            blk.5.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  176:          blk.5.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  177:          blk.5.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  178:            blk.5.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  179:          blk.5.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  180:          blk.5.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  181:            blk.5.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  182:          blk.5.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  183:          blk.5.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  184:            blk.5.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  185:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  186:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  187:          blk.6.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  188:          blk.6.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  189:            blk.6.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  190:          blk.6.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  191:          blk.6.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  192:            blk.6.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  193:          blk.6.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  194:          blk.6.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  195:            blk.6.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  196:          blk.6.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  197:          blk.6.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  198:            blk.6.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  199:          blk.6.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  200:          blk.6.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  201:            blk.6.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  202:          blk.6.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  203:          blk.6.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  204:        blk.6.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  205:              blk.6.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  206:         blk.6.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  207:              blk.6.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  208:              blk.6.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  209:            blk.6.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  210:          blk.6.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  211:          blk.6.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  212:            blk.6.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  213:          blk.6.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  214:          blk.6.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  215:            blk.6.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  216:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  217:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  218:          blk.7.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  219:          blk.7.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  220:            blk.7.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  221:          blk.7.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  222:          blk.7.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  223:            blk.7.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  224:          blk.7.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  225:          blk.7.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  226:            blk.7.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  227:          blk.7.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  228:          blk.7.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  229:            blk.7.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  230:          blk.7.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  231:          blk.7.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  232:            blk.7.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  233:          blk.7.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  234:          blk.7.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  235:            blk.7.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  236:          blk.7.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  237:          blk.7.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  238:            blk.7.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  239:          blk.7.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  240:          blk.7.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  241:            blk.7.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  242:        blk.7.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  243:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  244:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  245:              blk.7.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  246:         blk.7.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  247:              blk.7.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  248:              blk.7.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  249:          blk.8.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  250:          blk.8.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  251:            blk.8.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  252:          blk.8.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  253:          blk.8.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  254:            blk.8.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  255:          blk.8.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  256:          blk.8.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  257:            blk.8.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  258:          blk.8.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  259:        blk.8.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  260:              blk.8.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  261:         blk.8.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  262:              blk.8.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  263:              blk.8.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  264:         blk.10.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  265:         blk.10.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  266:           blk.10.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  267:       blk.10.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  268:             blk.10.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  269:        blk.10.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  270:             blk.10.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  271:             blk.10.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  272:          blk.8.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  273:            blk.8.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  274:          blk.8.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  275:          blk.8.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  276:            blk.8.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  277:          blk.8.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  278:          blk.8.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  279:            blk.8.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  280:          blk.8.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  281:          blk.8.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  282:            blk.8.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  283:          blk.8.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  284:          blk.8.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  285:            blk.8.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  286:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  287:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  288:          blk.9.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  289:          blk.9.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  290:            blk.9.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  291:          blk.9.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  292:          blk.9.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  293:            blk.9.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  294:          blk.9.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  295:          blk.9.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  296:            blk.9.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  297:          blk.9.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  298:          blk.9.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  299:            blk.9.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  300:          blk.9.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  301:          blk.9.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  302:            blk.9.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  303:          blk.9.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  304:          blk.9.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  305:            blk.9.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  306:          blk.9.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  307:          blk.9.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  308:            blk.9.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  309:          blk.9.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  310:          blk.9.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  311:            blk.9.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  312:        blk.9.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  313:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  314:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  315:              blk.9.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  316:         blk.9.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  317:              blk.9.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  318:              blk.9.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  319:         blk.10.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  320:         blk.10.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  321:           blk.10.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  322:         blk.10.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  323:         blk.10.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  324:           blk.10.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  325:         blk.10.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  326:         blk.10.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  327:           blk.10.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  328:         blk.10.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  329:         blk.10.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  330:           blk.10.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  331:         blk.10.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  332:         blk.10.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  333:           blk.10.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  334:         blk.10.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  335:         blk.10.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  336:           blk.10.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  337:         blk.10.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  338:         blk.10.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  339:           blk.10.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  340:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  341:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  342:         blk.11.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  343:         blk.11.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  344:           blk.11.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  345:         blk.11.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  346:         blk.11.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  347:           blk.11.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  348:         blk.11.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  349:         blk.11.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  350:           blk.11.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  351:         blk.11.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  352:         blk.11.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  353:           blk.11.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  354:         blk.11.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  355:         blk.11.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  356:           blk.11.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  357:         blk.11.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  358:         blk.11.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  359:           blk.11.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  360:         blk.11.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  361:         blk.11.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  362:       blk.11.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  363:             blk.11.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  364:        blk.11.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  365:             blk.11.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  366:             blk.11.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  367:           blk.11.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  368:         blk.11.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  369:         blk.11.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  370:           blk.11.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  371:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  372:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  373:         blk.12.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  374:         blk.12.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  375:           blk.12.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  376:         blk.12.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  377:         blk.12.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  378:           blk.12.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  379:         blk.12.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  380:         blk.12.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  381:           blk.12.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  382:         blk.12.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  383:         blk.12.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  384:           blk.12.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  385:         blk.12.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  386:         blk.12.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  387:           blk.12.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  388:         blk.12.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  389:         blk.12.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  390:           blk.12.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  391:         blk.12.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  392:         blk.12.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  393:           blk.12.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  394:         blk.12.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  395:         blk.12.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  396:           blk.12.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  397:       blk.12.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  398:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  399:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  400:             blk.12.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  401:        blk.12.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  402:             blk.12.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  403:             blk.12.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  404:         blk.13.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  405:         blk.13.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  406:           blk.13.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  407:         blk.13.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  408:         blk.13.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  409:           blk.13.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  410:         blk.13.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  411:         blk.13.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  412:           blk.13.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  413:         blk.13.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  414:         blk.13.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  415:           blk.13.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  416:         blk.13.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  417:       blk.13.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  418:             blk.13.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  419:        blk.13.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  420:             blk.13.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  421:             blk.13.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  422:         blk.13.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  423:           blk.13.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  424:         blk.13.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  425:         blk.13.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  426:           blk.13.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  427:         blk.13.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  428:         blk.13.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  429:           blk.13.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  430:         blk.13.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  431:         blk.13.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  432:           blk.13.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  433:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  434:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  435:         blk.14.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  436:         blk.14.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  437:           blk.14.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  438:         blk.14.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  439:         blk.14.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  440:           blk.14.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  441:         blk.14.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  442:         blk.14.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  443:           blk.14.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  444:         blk.14.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  445:         blk.14.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  446:           blk.14.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  447:         blk.14.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  448:         blk.14.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  449:           blk.14.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  450:         blk.14.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  451:         blk.14.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  452:           blk.14.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  453:         blk.14.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  454:         blk.14.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  455:           blk.14.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  456:         blk.14.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  457:         blk.14.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  458:           blk.14.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  459:       blk.14.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  460:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  461:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  462:             blk.14.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  463:        blk.14.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  464:             blk.14.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  465:             blk.14.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  466:         blk.15.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  467:         blk.15.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  468:           blk.15.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  469:         blk.15.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  470:         blk.15.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  471:           blk.15.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  472:       blk.15.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  473:             blk.15.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  474:        blk.15.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  475:             blk.15.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  476:             blk.15.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  477:         blk.15.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  478:         blk.15.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  479:           blk.15.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  480:         blk.15.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  481:         blk.15.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  482:           blk.15.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  483:         blk.15.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  484:         blk.15.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  485:           blk.15.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  486:         blk.15.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  487:         blk.15.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  488:           blk.15.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  489:         blk.15.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  490:         blk.15.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  491:           blk.15.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  492:         blk.15.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  493:         blk.15.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  494:           blk.15.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  495:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  496:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  497:         blk.16.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  498:         blk.16.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  499:           blk.16.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  500:         blk.16.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  501:         blk.16.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  502:           blk.16.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  503:         blk.16.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  504:         blk.16.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  505:           blk.16.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  506:         blk.16.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  507:         blk.16.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  508:           blk.16.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  509:         blk.16.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  510:         blk.16.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  511:           blk.16.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  512:         blk.16.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  513:         blk.16.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  514:           blk.16.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  515:         blk.16.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  516:         blk.16.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  517:           blk.16.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  518:         blk.16.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  519:         blk.16.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  520:       blk.16.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  521:             blk.16.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  522:        blk.16.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  523:             blk.16.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  524:             blk.16.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  525:           blk.16.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  526:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  527:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  528:         blk.17.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  529:         blk.17.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  530:           blk.17.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  531:         blk.17.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  532:         blk.17.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  533:           blk.17.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  534:         blk.17.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  535:         blk.17.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  536:           blk.17.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  537:         blk.17.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  538:         blk.17.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  539:           blk.17.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  540:         blk.17.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  541:         blk.17.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  542:           blk.17.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  543:         blk.17.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  544:         blk.17.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  545:           blk.17.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  546:         blk.17.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  547:         blk.17.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  548:           blk.17.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  549:         blk.17.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  550:         blk.17.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  551:           blk.17.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  552:       blk.17.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  553:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  554:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  555:             blk.17.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  556:        blk.17.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  557:             blk.17.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  558:             blk.17.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  559:         blk.18.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  560:         blk.18.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  561:           blk.18.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  562:         blk.18.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  563:         blk.18.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  564:           blk.18.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  565:         blk.18.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  566:         blk.18.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  567:           blk.18.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  568:         blk.18.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  569:         blk.18.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  570:           blk.18.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  571:         blk.18.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  572:         blk.18.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  573:           blk.18.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  574:         blk.18.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  575:       blk.18.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  576:             blk.18.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  577:        blk.18.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  578:             blk.18.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  579:             blk.18.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  580:         blk.18.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  581:           blk.18.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  582:         blk.18.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  583:         blk.18.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  584:           blk.18.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  585:         blk.18.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  586:         blk.18.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  587:           blk.18.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  588:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  589:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  590:         blk.19.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  591:         blk.19.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  592:           blk.19.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  593:         blk.19.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  594:         blk.19.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  595:           blk.19.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  596:         blk.19.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  597:         blk.19.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  598:           blk.19.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  599:         blk.19.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  600:         blk.19.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  601:           blk.19.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  602:         blk.19.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  603:         blk.19.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  604:           blk.19.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  605:         blk.19.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  606:         blk.19.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  607:           blk.19.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  608:         blk.19.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  609:         blk.19.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  610:           blk.19.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  611:         blk.19.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  612:         blk.19.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  613:           blk.19.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  614:       blk.19.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  615:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  616:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  617:             blk.19.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  618:        blk.19.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  619:             blk.19.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  620:             blk.19.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  621:         blk.20.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  622:         blk.20.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  623:           blk.20.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  624:         blk.20.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  625:         blk.20.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  626:           blk.20.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  627:         blk.20.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  628:         blk.20.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  629:           blk.20.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  630:       blk.20.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  631:             blk.20.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  632:        blk.20.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  633:             blk.20.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  634:             blk.20.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  635:         blk.20.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  636:         blk.20.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  637:           blk.20.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  638:         blk.20.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  639:         blk.20.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  640:           blk.20.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  641:         blk.20.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  642:         blk.20.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  643:           blk.20.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  644:         blk.20.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  645:         blk.20.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  646:           blk.20.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  647:         blk.20.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  648:         blk.20.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  649:           blk.20.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  650:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  651:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  652:         blk.21.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  653:         blk.21.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  654:           blk.21.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  655:         blk.21.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  656:         blk.21.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  657:           blk.21.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  658:         blk.21.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  659:         blk.21.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  660:           blk.21.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  661:         blk.21.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  662:         blk.21.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  663:           blk.21.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  664:         blk.21.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  665:         blk.21.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  666:           blk.21.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  667:         blk.21.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  668:         blk.21.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  669:           blk.21.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  670:         blk.21.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  671:         blk.21.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  672:           blk.21.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  673:         blk.21.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  674:         blk.21.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  675:           blk.21.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  676:       blk.21.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  677:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  678:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  679:             blk.21.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  680:        blk.21.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  681:             blk.21.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  682:             blk.21.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  683:         blk.22.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  684:         blk.22.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  685:       blk.22.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  686:             blk.22.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  687:        blk.22.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  688:             blk.22.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  689:             blk.22.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  690:           blk.22.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  691:         blk.22.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  692:         blk.22.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  693:           blk.22.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  694:         blk.22.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  695:         blk.22.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  696:           blk.22.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  697:         blk.22.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  698:         blk.22.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  699:           blk.22.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  700:         blk.22.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  701:         blk.22.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  702:           blk.22.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  703:         blk.22.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  704:         blk.22.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  705:           blk.22.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  706:         blk.22.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  707:         blk.22.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  708:           blk.22.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  709:         blk.22.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  710:         blk.22.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  711:           blk.22.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  712:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  713:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  714:         blk.23.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  715:         blk.23.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  716:           blk.23.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  717:         blk.23.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  718:         blk.23.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  719:           blk.23.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  720:         blk.23.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  721:         blk.23.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  722:           blk.23.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  723:         blk.23.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  724:         blk.23.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  725:           blk.23.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  726:         blk.23.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  727:         blk.23.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  728:           blk.23.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  729:         blk.23.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  730:         blk.23.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  731:           blk.23.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  732:         blk.23.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  733:       blk.23.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  734:             blk.23.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  735:        blk.23.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  736:             blk.23.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  737:             blk.23.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  738:         blk.23.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  739:           blk.23.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  740:         blk.23.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  741:         blk.23.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  742:           blk.23.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  743:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  744:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  745:         blk.24.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  746:         blk.24.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  747:           blk.24.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  748:         blk.24.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  749:         blk.24.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  750:           blk.24.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  751:         blk.24.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  752:         blk.24.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  753:           blk.24.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  754:         blk.24.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  755:         blk.24.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  756:           blk.24.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  757:         blk.24.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  758:         blk.24.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  759:           blk.24.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  760:         blk.24.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  761:         blk.24.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  762:           blk.24.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  763:         blk.24.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  764:         blk.24.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  765:           blk.24.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  766:         blk.24.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  767:         blk.24.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  768:           blk.24.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  769:       blk.24.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  770:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  771:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  772:             blk.24.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  773:        blk.24.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  774:             blk.24.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  775:             blk.24.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  776:         blk.25.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  777:         blk.25.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  778:           blk.25.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  779:         blk.25.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  780:         blk.25.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  781:           blk.25.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  782:         blk.25.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  783:         blk.25.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  784:           blk.25.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  785:         blk.25.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  786:         blk.25.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  787:           blk.25.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  788:       blk.25.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  789:             blk.25.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  790:        blk.25.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  791:             blk.25.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  792:             blk.25.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  793:         blk.25.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  794:         blk.25.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  795:           blk.25.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  796:         blk.25.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  797:         blk.25.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  798:           blk.25.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  799:         blk.25.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  800:         blk.25.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  801:           blk.25.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  802:         blk.25.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  803:         blk.25.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  804:           blk.25.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  805:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  806:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  807:         blk.26.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  808:         blk.26.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  809:           blk.26.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  810:         blk.26.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  811:         blk.26.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  812:           blk.26.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  813:         blk.26.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  814:         blk.26.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  815:           blk.26.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  816:         blk.26.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  817:         blk.26.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  818:           blk.26.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  819:         blk.26.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  820:         blk.26.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  821:           blk.26.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  822:         blk.26.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  823:         blk.26.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  824:           blk.26.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  825:         blk.26.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  826:         blk.26.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  827:           blk.26.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  828:         blk.26.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  829:         blk.26.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  830:           blk.26.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  831:       blk.26.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  832:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  833:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  834:             blk.26.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  835:        blk.26.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  836:             blk.26.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  837:             blk.26.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  838:         blk.27.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  839:         blk.27.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  840:           blk.27.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  841:         blk.27.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  842:         blk.27.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  843:       blk.27.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  844:             blk.27.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  845:        blk.27.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  846:             blk.27.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  847:             blk.27.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  848:           blk.27.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  849:         blk.27.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  850:         blk.27.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  851:           blk.27.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  852:         blk.27.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  853:         blk.27.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  854:           blk.27.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  855:         blk.27.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  856:         blk.27.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  857:           blk.27.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  858:         blk.27.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  859:         blk.27.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  860:           blk.27.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  861:         blk.27.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  862:         blk.27.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  863:           blk.27.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  864:         blk.27.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  865:         blk.27.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  866:           blk.27.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  867:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  868:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  869:         blk.28.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  870:         blk.28.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  871:           blk.28.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  872:         blk.28.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  873:         blk.28.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  874:           blk.28.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  875:         blk.28.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  876:         blk.28.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  877:           blk.28.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  878:         blk.28.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  879:         blk.28.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  880:           blk.28.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  881:         blk.28.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  882:         blk.28.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  883:           blk.28.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  884:         blk.28.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  885:         blk.28.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  886:           blk.28.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  887:         blk.28.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  888:         blk.28.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  889:           blk.28.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  890:         blk.28.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  891:       blk.28.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  892:             blk.28.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  893:        blk.28.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  894:             blk.28.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  895:             blk.28.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  896:         blk.28.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  897:           blk.28.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  898:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  899:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  900:         blk.29.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  901:         blk.29.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  902:           blk.29.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  903:         blk.29.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  904:         blk.29.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  905:           blk.29.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  906:         blk.29.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  907:         blk.29.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  908:           blk.29.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  909:         blk.29.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  910:         blk.29.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  911:           blk.29.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  912:         blk.29.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  913:         blk.29.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  914:           blk.29.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  915:         blk.29.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  916:         blk.29.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  917:           blk.29.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  918:         blk.29.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  919:         blk.29.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  920:           blk.29.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  921:         blk.29.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  922:         blk.29.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  923:           blk.29.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  924:       blk.29.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  925:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  926:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  927:             blk.29.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  928:        blk.29.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  929:             blk.29.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  930:             blk.29.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  931:         blk.30.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  932:         blk.30.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  933:           blk.30.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  934:         blk.30.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  935:         blk.30.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  936:           blk.30.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  937:         blk.30.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  938:         blk.30.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  939:           blk.30.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  940:         blk.30.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  941:         blk.30.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  942:           blk.30.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  943:         blk.30.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  944:         blk.30.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  945:           blk.30.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  946:       blk.30.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  947:             blk.30.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  948:        blk.30.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  949:             blk.30.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  950:             blk.30.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  951:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n","llama_model_loader: - tensor  952:         blk.30.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  953:         blk.30.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  954:           blk.30.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  955:         blk.30.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  956:         blk.30.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  957:           blk.30.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  958:         blk.30.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  959:         blk.30.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  960:           blk.30.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  961:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  962:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  963:         blk.31.ffn_gate.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  964:         blk.31.ffn_down.0.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  965:           blk.31.ffn_up.0.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  966:         blk.31.ffn_gate.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  967:         blk.31.ffn_down.1.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  968:           blk.31.ffn_up.1.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  969:         blk.31.ffn_gate.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  970:         blk.31.ffn_down.2.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  971:           blk.31.ffn_up.2.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  972:         blk.31.ffn_gate.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  973:         blk.31.ffn_down.3.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  974:           blk.31.ffn_up.3.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  975:         blk.31.ffn_gate.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  976:         blk.31.ffn_down.4.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  977:           blk.31.ffn_up.4.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  978:         blk.31.ffn_gate.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  979:         blk.31.ffn_down.5.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  980:           blk.31.ffn_up.5.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  981:         blk.31.ffn_gate.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  982:         blk.31.ffn_down.6.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  983:           blk.31.ffn_up.6.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  984:         blk.31.ffn_gate.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  985:         blk.31.ffn_down.7.weight q5_0     [ 14336,  4096,     1,     1 ]\n","llama_model_loader: - tensor  986:           blk.31.ffn_up.7.weight q5_0     [  4096, 14336,     1,     1 ]\n","llama_model_loader: - tensor  987:       blk.31.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n","llama_model_loader: - tensor  988:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  989:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: - tensor  990:             blk.31.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  991:        blk.31.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  992:             blk.31.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n","llama_model_loader: - tensor  993:             blk.31.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n","llama_model_loader: - tensor  994:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-v0.1\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n","llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n","llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  13:                          general.file_type u32              = 8\n","llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type  f16:   32 tensors\n","llama_model_loader: - type q5_0:  833 tensors\n","llama_model_loader: - type q8_0:   64 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 32768\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 8\n","llm_load_print_meta: n_expert_used    = 2\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 1000000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 32768\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = mostly Q5_0\n","llm_load_print_meta: model params     = 46.70 B\n","llm_load_print_meta: model size       = 30.02 GiB (5.52 BPW) \n","llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-v0.1\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.39 MiB\n","llm_load_tensors: using CUDA for GPU acceleration\n","llm_load_tensors: mem required  = 2098.07 MiB\n","llm_load_tensors: offloading 30 repeating layers to GPU\n","llm_load_tensors: offloaded 30/33 layers to GPU\n","llm_load_tensors: VRAM used: 28637.81 MiB\n","..................................................................................................\n","CUDA error 2 at ggml-cuda.cu:8556: out of memory\n","current device: 0\n","GGML_ASSERT: ggml-cuda.cu:8556: !\"CUDA error\"\n"]}],"source":["%cd /kaggle/llama.cpp\n","!pwd\n","# !cat server.log\n","!tail -f server.log\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
